{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGNMKUFOLyTaY4xJggBYj2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Backpropagation at the backend"],"metadata":{"id":"hfwNYTtFvPM2"}},{"cell_type":"markdown","source":["# Part 1: Setup boilerplate code"],"metadata":{"id":"KyPzkaWj4hDn"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ndo3EypR0H73","executionInfo":{"status":"ok","timestamp":1726698664090,"user_tz":240,"elapsed":5489,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["# Get the dataset\n","!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sa1WMIi1vwyR","executionInfo":{"status":"ok","timestamp":1726698664090,"user_tz":240,"elapsed":6,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"bfc71f68-1b17-47f4-eb3f-e5f125d4a218"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-18 22:31:03--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228145 (223K) [text/plain]\n","Saving to: ‘names.txt.2’\n","\n","\rnames.txt.2           0%[                    ]       0  --.-KB/s               \rnames.txt.2         100%[===================>] 222.80K  --.-KB/s    in 0.008s  \n","\n","2024-09-18 22:31:03 (28.2 MB/s) - ‘names.txt.2’ saved [228145/228145]\n","\n"]}]},{"cell_type":"code","source":["# Read all the dataset and show first 8 names\n","words = open('names.txt', 'r').read().splitlines()\n","print(len(words))\n","print(max(len(w) for w in words))\n","print(words[:8])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwbjLcXMv2SG","executionInfo":{"status":"ok","timestamp":1726698664279,"user_tz":240,"elapsed":192,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"0981a16b-fcc5-4305-831a-d3d113eb2209"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["32033\n","15\n","['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"]}]},{"cell_type":"code","source":["# Building position to character mapping\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(itos)\n","print(itos)\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5BwXfq7wKPv","executionInfo":{"status":"ok","timestamp":1726698664280,"user_tz":240,"elapsed":2,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"7c79e93a-286e-46ef-918d-6490576afc49"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n","27\n"]}]},{"cell_type":"code","source":["# Building the dataset with context length\n","block_size = 3 # character context for next character prediction, so last 3 character is used to predict next character\n","\n","def build_dataset(words):\n","  X, Y = [], []\n","  for w in words:\n","    context = [0] * block_size # Initially context start with [0,0,0] which is [...]\n","    for ch in w + '.': # . is the ending character for each word\n","      ix = stoi[ch]\n","      X.append(context)\n","      Y.append(ix)\n","      context = context[1:] + [ix] # Rolling - the first character of context is removed and the next character is added from word [...]->[..e]\n","  X = torch.tensor(X) # context for train [[...], [..e], [.em]], obviously X contains pos not characters\n","  Y = torch.tensor(Y) # next character pred [e, m, m], obviously Y contains pos not characters\n","  print(X.shape, Y.shape)\n","  return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr, Ytr = build_dataset(words[:n1]) # Train split - 80%\n","Xdev, Ydev = build_dataset(words[n1:n2]) # Dev/Val split - 10%\n","Xte, Yte = build_dataset(words[n2:]) # Test split - 10%"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhlWpx0DxoPY","executionInfo":{"status":"ok","timestamp":1726698666333,"user_tz":240,"elapsed":1700,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"e6b7a843-bffc-4399-da38-880ce29f4168"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n"]}]},{"cell_type":"code","source":["# Utility function to compare out gradient calculation to Pytorch implementation - Unit test\n","def test_gradients_cmp(s, dt, t):\n","  # s - spaces needed, dt - Our grad implementation, t - Pytorch grad implementation\n","  ex = torch.all(dt == t.grad).item() # Check if all values match\n","  app = torch.allclose(dt, t.grad) # Check if values approximately match\n","  maxdiff = (dt - t.grad).abs().max().item() # What is the max diff of any element between the implementation\n","  print(f'{s:15s} | Exact: {str(ex):5s} | Approx: {str(app):5s} | Maxdiff: {maxdiff}')"],"metadata":{"id":"3bKfvBTox3wW","executionInfo":{"status":"ok","timestamp":1726698666333,"user_tz":240,"elapsed":5,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Lets define the model layers in torch\n","n_embed = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 64 # the number of neurons in the hidden layer\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C = torch.randn((vocab_size, n_embed), generator=g)\n","\n","# Layer 1\n","W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * (5/3)/((n_embed * block_size)**0.5)\n","b1 = torch.randn(n_hidden, generator=g) * 0.1\n","\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n","b2 = torch.randn(vocab_size, generator=g) * 0.1\n","\n","# Batch Normalization parameters\n","bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden)) * 0.1\n","\n","# Note: A lot of non standard initialization since all zero initialization may mask incorrect backward pass implementation\n","# List of parameters to optimize\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # Number of paramters in total\n","for p in parameters:\n","  p.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxUTGmW4zcYX","executionInfo":{"status":"ok","timestamp":1726698666333,"user_tz":240,"elapsed":4,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"60ad5c8e-d580-4f3b-b069-4eb6589e6731"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}]},{"cell_type":"code","source":["# Creating our single iteration mini batch\n","batch_size = 32\n","n = batch_size # Shorter variable name for convinence\n","# Constructing a minibatch\n","ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","Xb, Yb = Xtr[ix], Ytr[ix] # Batch of inputs and targets"],"metadata":{"id":"v5d0Ya-fSGyS","executionInfo":{"status":"ok","timestamp":1726698666333,"user_tz":240,"elapsed":3,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Forward pass \"chunkated\" so that our manual implementation of backward pass can be easily done one at a time\n","emb = C[Xb] # Get the embedding for entire minibatch\n","embcat = emb.view(emb.shape[0], -1) # Flatten the embedding, last 2 dimensions of embedding is flattened so its (train_size, context_length)\n","\n","# Linear Layer 1\n","hprebn = embcat @ W1 + b1 # Calculate the hidden layer pre activation\n","\n","# BatchNorm Layer gamma * (x - mean)/sqrt(std^2+eps) + beta, gamma = gain and beta = bias\n","bnmean = 1/n*hprebn.sum(0, keepdim=True) # Get the mean\n","bndiff = hprebn - bnmean\n","bndiff2 = bndiff**2\n","bnvar = 1/(n-1)*bndiff2.sum(0, keepdim=True) # Note: Bessel's correction (dividing by n-1, not n) and std^2 is var\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hpreact = bngain * bnraw + bnbias\n","\n","# Non Linearity\n","h = torch.tanh(hpreact) # Calculate the hidden layer activation\n","\n","# Linear Layer 2\n","logits = h @ W2 + b2 # Calculate the output layer pre activation\n","\n","# Loss calculation\n","# For numerical stability we subtract the max logit value, basically normalizing logits\n","logits_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logits_maxes\n","# Getting loss\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean() # Taking the mean of all log probs and negating it\n","\n","# Backward Pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum_inv, counts_sum, # afaik there is no cleaner way\n","          norm_logits, logits_maxes, logits, h, hpreact, bnraw,\n","          bnvar_inv, bnvar, bndiff2, bndiff, bnmean, hprebn,\n","          embcat, emb]:\n","          t.retain_grad()\n","loss.backward()\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fa50VhzMTmGG","executionInfo":{"status":"ok","timestamp":1726698666500,"user_tz":240,"elapsed":170,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"87ad27ba-403d-44c6-9f84-75faa766c3a7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.3480, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Part 2: Backpropagation Manual Implementation"],"metadata":{"id":"34yWc5MoZxNL"}},{"cell_type":"code","source":["# Loss Backprop\n","dlogprobs = torch.zeros_like(logprobs) # dloss/dlogprobs\n","dlogprobs[range(n), Yb] = -1.0/n\n","\n","dprobs = 1.0/probs * dlogprobs # dlogprobs/dprobs, dlogprobs comes from chain rule\n","dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # dprobs/dcounts_sum_inv, dprobs comes from chain rule, sum(1,keepdim_true), since counts * counts_sum_inv have shape (32,27) and (32,1) so to take of broadcasting\n","dcounts = counts_sum_inv * dprobs # dprobs/dcounts, dcounts comes from chain rule\n","dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # dcounts_sum_inv/dcounts_sum, dcounts_sum_inv comes from chain rule\n","dcounts += torch.ones_like(counts) * dcounts_sum # dcounts_sum/dcounts, dcounts_sum comes from chain rule, since this is row wise addition the gradient just passes equally to all summing elements, so counts size and grad was 1\n","dnorm_logits = counts * dcounts # dcounts/dnorm_logits, dcounts comes from chain rule\n","dlogits = 1.0 * dnorm_logits.clone() # dnorm_logits/dlogits, dnorm_logits comes from chain rule\n","dlogit_maxes = (-1.0 * dnorm_logits).sum(1, keepdim=True) # dnorm_logits/dlogit_maxes, dnorm_logits comes from chain rule, since dlogit_maxes is 32,1 dim\n","dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # dlogit_maxes/dlogits, dlogit_maxes comes from chain rule, we get index of max logit and pass through one hot and product with dlogit_max means the max logit will be impact by gradient change\n","\n","test_gradients_cmp('logprobs', dlogprobs, logprobs)\n","test_gradients_cmp('probs', dprobs, probs)\n","test_gradients_cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","test_gradients_cmp('counts_sum', dcounts_sum, counts_sum)\n","test_gradients_cmp('counts', dcounts, counts)\n","test_gradients_cmp('norm_logits', dnorm_logits, norm_logits)\n","test_gradients_cmp('logit_maxes', dlogit_maxes, logits_maxes)\n","test_gradients_cmp('logits', dlogits, logits)\n","\n","# Linear Layer 2 Backprop\n","dh = dlogits @ W2.T # dlogits/dh, dlogits comes from chain rule\n","dW2 = h.T @ dlogits # dlogits/dW2, dlogits comes from chain rule\n","db2 = dlogits.sum(0) # dlogits/db2, dlogits comes from chain rule\n","\n","test_gradients_cmp('h', dh, h)\n","test_gradients_cmp('W2', dW2, W2)\n","test_gradients_cmp('b2', db2, b2)\n","\n","# Non Linearity Backprop\n","dhpreact = (1.0 - h**2) * dh # dh/dhpreact, dh comes from chain rule\n","\n","test_gradients_cmp('hpreact', dhpreact, hpreact)\n","\n","# Batch Norm Backprop\n","dbngain = (bnraw * dhpreact).sum(0, keepdim=True) # dhpreact/dbngain, dhpreact comes from chain rule, again broadcast bngain - (1,64), [bnraw - (32,64), bnbias - (1,64)], * - element wise multiply\n","dbnraw = bngain * dhpreact # dhpreact/dbnraw, dhpreact comes from chain rule\n","dbnbias = dhpreact.sum(0, keepdim=True) # dhpreact/dbnbias, dhpreact comes from chain rule\n","dbndiff = bnvar_inv * dbnraw # dbnraw/dbdiff, dbnraw comes from chain rule\n","dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) # dbnraw/dbvar_inv, dbnraw comes from chain rule, bnvar_inv - (1,64)\n","dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv # dbnvar_inv/dbnvar, dbnvar_inv comes from chain rule\n","dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar # dbnvar/dbdiff2, dbnvar comes from chain rule\n","dbndiff += (2*bndiff) * dbndiff2 # dbndiff2/dbdiff, dbndiff2 comes from chain rule\n","dbnmean = -1.0*dbndiff.sum(0) # dbndiff/dbnmean, dbndiff comes from chain rule, bnmean - (1,64)\n","dhprebn = 1.0*dbndiff.clone() # dbndiff/dhprebn, dbndiff comes from chain rule\n","dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmean) # dbnmean/dhprebn, dbnmean comes from chain rule, hprebn - (32,64)\n","\n","test_gradients_cmp('bngain', dbngain, bngain)\n","test_gradients_cmp('bnraw', dbnraw, bnraw)\n","test_gradients_cmp('bnbias', dbnbias, bnbias)\n","test_gradients_cmp('bndiff', dbndiff, bndiff)\n","test_gradients_cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","test_gradients_cmp('bnvar', dbnvar, bnvar)\n","test_gradients_cmp('bndiff2', dbndiff2, bndiff2)\n","test_gradients_cmp('bnmean', dbnmean, bnmean)\n","test_gradients_cmp('hprebn', dhprebn, hprebn)\n","\n","# Linear Layer 1 Backprop\n","dembcat = dhprebn @ W1.T # dhprebn/dembcat, dhprebn comes from chain rule\n","dW1 = embcat.T @ dhprebn # dhprebn/dW1, dhprebn comes from chain rule\n","db1 = dhprebn.sum(0) # dhprebn/db1, dhprebn comes from chain rule\n","\n","test_gradients_cmp('embcat', dembcat, embcat)\n","test_gradients_cmp('W1', dW1, W1)\n","test_gradients_cmp('b1', db1, b1)\n","\n","# Embedding Backprop\n","demb = dembcat.view(emb.shape[0], emb.shape[1], -1) # dembcat/demb, dembcat comes from chain rule emb - (32,3,10), embcat (32,30)\n","dC = torch.zeros_like(C) # demb/dC, demb comes from chain rule\n","for k in range(Xb.shape[0]): # Xb contains all the minibatch train indices\n","  for j in range(Xb.shape[1]):\n","    ix = Xb[k,j]\n","    dC[ix] += demb[k,j]\n","\n","test_gradients_cmp('emb', demb, emb)\n","test_gradients_cmp('C', dC, C)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlCDRf0WZuy1","executionInfo":{"status":"ok","timestamp":1726698668112,"user_tz":240,"elapsed":377,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"ef7e4a96-0a49-43f1-b5f7-45edbbd52148"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["logprobs        | Exact: True  | Approx: True  | Maxdiff: 0.0\n","probs           | Exact: True  | Approx: True  | Maxdiff: 0.0\n","counts_sum_inv  | Exact: True  | Approx: True  | Maxdiff: 0.0\n","counts_sum      | Exact: True  | Approx: True  | Maxdiff: 0.0\n","counts          | Exact: True  | Approx: True  | Maxdiff: 0.0\n","norm_logits     | Exact: True  | Approx: True  | Maxdiff: 0.0\n","logit_maxes     | Exact: True  | Approx: True  | Maxdiff: 0.0\n","logits          | Exact: True  | Approx: True  | Maxdiff: 0.0\n","h               | Exact: True  | Approx: True  | Maxdiff: 0.0\n","W2              | Exact: True  | Approx: True  | Maxdiff: 0.0\n","b2              | Exact: True  | Approx: True  | Maxdiff: 0.0\n","hpreact         | Exact: False | Approx: True  | Maxdiff: 4.656612873077393e-10\n","bngain          | Exact: False | Approx: True  | Maxdiff: 3.725290298461914e-09\n","bnraw           | Exact: False | Approx: True  | Maxdiff: 4.656612873077393e-10\n","bnbias          | Exact: False | Approx: True  | Maxdiff: 3.725290298461914e-09\n","bndiff          | Exact: False | Approx: True  | Maxdiff: 4.656612873077393e-10\n","bnvar_inv       | Exact: False | Approx: True  | Maxdiff: 3.725290298461914e-09\n","bnvar           | Exact: False | Approx: True  | Maxdiff: 9.313225746154785e-10\n","bndiff2         | Exact: False | Approx: True  | Maxdiff: 2.9103830456733704e-11\n","bnmean          | Exact: False | Approx: True  | Maxdiff: 1.862645149230957e-09\n","hprebn          | Exact: False | Approx: True  | Maxdiff: 4.656612873077393e-10\n","embcat          | Exact: False | Approx: True  | Maxdiff: 1.862645149230957e-09\n","W1              | Exact: False | Approx: True  | Maxdiff: 3.725290298461914e-09\n","b1              | Exact: False | Approx: True  | Maxdiff: 2.7939677238464355e-09\n","emb             | Exact: False | Approx: True  | Maxdiff: 1.862645149230957e-09\n","C               | Exact: False | Approx: True  | Maxdiff: 3.4924596548080444e-09\n"]}]},{"cell_type":"markdown","source":["# Part 3: Cross Entropy and Batch Norm One Go Backpropagation Manual Implementation"],"metadata":{"id":"IAR3rugtaD46"}},{"cell_type":"code","source":["# Cross Entropy short backprop implementation\n","dlogits = F.softmax(logits, 1) # Getting softmax for each train example across row axis, which is like probability\n","dlogits[range(n), Yb] -= 1 # Now grad is prob_i if i!=y  and prob_i-1 if i==y\n","dlogits /= n # Now propagate 1/n (averageness) aross all\n","\n","test_gradients_cmp('logits', dlogits, logits)\n","\n","# Batch Norm short backprop implementation\n","dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","\n","test_gradients_cmp('hprebn', dhprebn, hprebn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTen0fydyMzj","executionInfo":{"status":"ok","timestamp":1726698669465,"user_tz":240,"elapsed":180,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"4b91a9b7-665b-46e7-f1f5-566ff7ca48ff"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["logits          | Exact: False | Approx: True  | Maxdiff: 7.2177499532699585e-09\n","hprebn          | Exact: False | Approx: True  | Maxdiff: 9.313225746154785e-10\n"]}]},{"cell_type":"markdown","source":["# Part 4: Bringing it all together"],"metadata":{"id":"YYaXaMlyaVb8"}},{"cell_type":"code","source":["# Setting up the model\n","n_embed = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C = torch.randn((vocab_size, n_embed), generator=g)\n","\n","# Layer 1\n","W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * (5/3)/((n_embed * block_size)**0.5)\n","b1 = torch.randn(n_hidden, generator=g) * 0.1\n","\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n","b2 = torch.randn(vocab_size, generator=g) * 0.1\n","\n","# Batch Normalization parameters\n","bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden)) * 0.1\n","\n","# Note: A lot of non standard initialization since all zero initialization may mask incorrect backward pass implementation\n","# List of parameters to optimize\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # Number of paramters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# Training and Optimization\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size\n","lossi = [] # Stores the loss after every step\n","\n","with torch.no_grad():\n","  for i in range(max_steps):\n","\n","    # Construct a minibatch\n","    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb, Yb = Xtr[ix], Ytr[ix]\n","\n","    # Forward pass\n","    emb = C[Xb] # Get the embedding for entire minibatch\n","    embcat = emb.view(emb.shape[0], -1) # Flatten the embedding, last 2 dimensions of embedding is flattened so its (train_size, context_length)\n","\n","    # Linear Layer\n","    hprebn = embcat @ W1 + b1 # Calculate the hidden layer pre activation\n","\n","    # BatchNorm Layer\n","    bnmean = hprebn.mean(0, keepdim=True) # Get the mean\n","    bnvar = hprebn.var(0, keepdim=True, unbiased=True) # Get the variance\n","    bnvar_inv = (bnvar + 1e-5)**-0.5\n","    bnraw = (hprebn - bnmean) * bnvar_inv\n","    hpreact = bngain * bnraw + bnbias\n","\n","    # Non Linearity\n","    h = torch.tanh(hpreact) # Calculate the hidden layer activation\n","\n","    # Linear Layer\n","    logits = h @ W2 + b2 # Calculate the output layer preactivation\n","\n","    # Loss\n","    loss = F.cross_entropy(logits, Yb)\n","\n","    # Backward Pass\n","    for p in parameters:\n","      p.grad = None\n","    # loss.backward()\n","\n","    # Manual Backpropagation\n","    # Loss Backprop\n","    dlogits = F.softmax(logits, 1)\n","    dlogits[range(n), Yb] -= 1\n","    dlogits /= n\n","\n","    # Linear Layer 2 Backprop\n","    dh = dlogits @ W2.T\n","    dW2 = h.T @ dlogits\n","    db2 = dlogits.sum(0)\n","\n","    # Non Linearity Backprop\n","    dhpreact = (1.0 - h**2) * dh\n","\n","    # Batch Norm Backprop\n","    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","    dbnbias = dhpreact.sum(0, keepdim=True)\n","    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","\n","    # Linear Layer 1 Backprop\n","    dembcat = dhprebn @ W1.T\n","    dW1 = embcat.T @ dhprebn\n","    db1 = dhprebn.sum(0)\n","\n","    # Embedding Backprop\n","    demb = dembcat.view(emb.shape[0], emb.shape[1], -1)\n","    dC = torch.zeros_like(C)\n","    for k in range(Xb.shape[0]):\n","      for j in range(Xb.shape[1]):\n","        ix = Xb[k,j]\n","        dC[ix] += demb[k,j]\n","\n","    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","\n","    # Update\n","    lr = 0.1 if i < 100000 else 0.01 # Learning rate decay\n","    for p,grad in zip(parameters,grads):\n","      # p.data += -lr * p.grad\n","      p.data += -lr * grad\n","\n","    # Track stats\n","    if i % 10000 == 0: # Print every once every 10k steps\n","      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","\n","    # if i >= 100:\n","    #   break # After Debug: Remove this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRj0pFavp6gf","executionInfo":{"status":"ok","timestamp":1726699985859,"user_tz":240,"elapsed":731999,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"615235f8-a012-43b1-aa6f-6ee05bf2891f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n","      0/ 200000: 3.7880\n","  10000/ 200000: 2.1641\n","  20000/ 200000: 2.3662\n","  30000/ 200000: 2.4217\n","  40000/ 200000: 1.9684\n","  50000/ 200000: 2.4886\n","  60000/ 200000: 2.4104\n","  70000/ 200000: 2.0820\n","  80000/ 200000: 2.4035\n","  90000/ 200000: 2.1479\n"," 100000/ 200000: 1.9805\n"," 110000/ 200000: 2.2685\n"," 120000/ 200000: 1.9944\n"," 130000/ 200000: 2.3934\n"," 140000/ 200000: 2.2971\n"," 150000/ 200000: 2.2150\n"," 160000/ 200000: 1.9154\n"," 170000/ 200000: 1.9014\n"," 180000/ 200000: 2.0973\n"," 190000/ 200000: 1.9360\n"]}]},{"cell_type":"code","source":["# Compare the gradients to torch grads [Dont run this after debug]\n","for p, g in zip(parameters, grads):\n","  test_gradients_cmp(str(tuple(p.shape)), g, p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4JvfN52OsAG","executionInfo":{"status":"ok","timestamp":1726699052248,"user_tz":240,"elapsed":169,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"8f1fd5a9-29be-49a9-d4f7-74a545fba7ed"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["(27, 10)        | Exact: False | Approx: True  | Maxdiff: 1.1175870895385742e-08\n","(30, 200)       | Exact: False | Approx: True  | Maxdiff: 9.313225746154785e-09\n","(200,)          | Exact: False | Approx: True  | Maxdiff: 3.725290298461914e-09\n","(200, 27)       | Exact: False | Approx: True  | Maxdiff: 1.4901161193847656e-08\n","(27,)           | Exact: False | Approx: True  | Maxdiff: 7.450580596923828e-09\n","(1, 200)        | Exact: False | Approx: True  | Maxdiff: 2.7939677238464355e-09\n","(1, 200)        | Exact: False | Approx: True  | Maxdiff: 7.450580596923828e-09\n"]}]},{"cell_type":"code","source":["# Calaibrate batch norm paramters at end of training\n","with torch.no_grad():\n","  # Pass training set through\n","  emb = C[Xtr]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = embcat @ W1\n","  # Measure the mean/std over the entire training set\n","  bnmean = hpreact.mean(0, keepdim=True)\n","  bnstd = hpreact.var(0, keepdim=True, unbiased=True)"],"metadata":{"id":"ghZnETNYO06M","executionInfo":{"status":"ok","timestamp":1726700220432,"user_tz":240,"elapsed":1376,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Calculate loss for different split after tarining\n","@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","    'train': (Xtr, Ytr),\n","    'val': (Xdev, Ydev),\n","    'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x] # (split_size, block_size, n_embd)\n","  embcat = emb.view(emb.shape[0], -1) # concat into (split_size, block_size * n_embd)\n","  hpreact = embcat @ W1 + b1 # (split_size, n_hidden)\n","  hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n","  h = torch.tanh(hpreact) # (split_size, n_hidden)\n","  logits = h @ W2 + b2 # (split_size, vocab_size)\n","  loss = F.cross_entropy(logits, y) # BTW, negative log likelihood of softmax is cross entropy\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')\n","split_loss('test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQ8qg9rITXNP","executionInfo":{"status":"ok","timestamp":1726700223572,"user_tz":240,"elapsed":1731,"user":{"displayName":"Aryan Singh","userId":"02920260528401427458"}},"outputId":"31cc6a8e-1700-45fd-81e5-ab497e7c1b08"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["train 2.376091241836548\n","val 2.3904130458831787\n","test 2.3936409950256348\n"]}]},{"cell_type":"code","source":["# Make predictions from the model\n","g = torch.Generator().manual_seed(21474836)\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      emb = C[torch.tensor([context])] # (1,block_size,d) -> (1,3,10)\n","      h = torch.tanh(emb.view(1, -1) @ W1 + b1) # There is only example so 1, and (1,-1) can be writen as (1,30)\n","      logits = h @ W2 + b2\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","\n","    print(''.join(itos[i] for i in out))"],"metadata":{"id":"fa7pgtMVTv6f"},"execution_count":null,"outputs":[]}]}